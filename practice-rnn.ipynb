{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 846: expected 3 fields, saw 4\\nSkipping line 904: expected 3 fields, saw 4\\nSkipping line 914: expected 3 fields, saw 4\\nSkipping line 1264: expected 3 fields, saw 4\\nSkipping line 1269: expected 3 fields, saw 4\\nSkipping line 1293: expected 3 fields, saw 4\\nSkipping line 1348: expected 3 fields, saw 4\\nSkipping line 1430: expected 3 fields, saw 4\\nSkipping line 1486: expected 3 fields, saw 4\\nSkipping line 1710: expected 3 fields, saw 4\\nSkipping line 2699: expected 3 fields, saw 4\\nSkipping line 2728: expected 3 fields, saw 4\\nSkipping line 3000: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 4015: expected 3 fields, saw 4\\nSkipping line 6118: expected 3 fields, saw 4\\nSkipping line 6354: expected 3 fields, saw 4\\nSkipping line 6429: expected 3 fields, saw 4\\nSkipping line 6528: expected 3 fields, saw 4\\nSkipping line 6930: expected 3 fields, saw 4\\nSkipping line 6944: expected 3 fields, saw 4\\nSkipping line 6948: expected 3 fields, saw 4\\nSkipping line 6949: expected 3 fields, saw 4\\nSkipping line 6951: expected 3 fields, saw 5\\nSkipping line 6954: expected 3 fields, saw 4\\nSkipping line 6956: expected 3 fields, saw 4\\nSkipping line 6965: expected 3 fields, saw 4\\nSkipping line 6984: expected 3 fields, saw 4\\nSkipping line 6988: expected 3 fields, saw 4\\nSkipping line 7020: expected 3 fields, saw 4\\nSkipping line 7049: expected 3 fields, saw 4\\nSkipping line 7058: expected 3 fields, saw 4\\nSkipping line 7118: expected 3 fields, saw 4\\nSkipping line 7126: expected 3 fields, saw 4\\nSkipping line 7131: expected 3 fields, saw 4\\nSkipping line 7135: expected 3 fields, saw 4\\nSkipping line 7163: expected 3 fields, saw 4\\nSkipping line 7177: expected 3 fields, saw 4\\nSkipping line 7223: expected 3 fields, saw 4\\nSkipping line 7240: expected 3 fields, saw 4\\nSkipping line 7241: expected 3 fields, saw 4\\nSkipping line 7277: expected 3 fields, saw 4\\nSkipping line 7289: expected 3 fields, saw 4\\nSkipping line 7293: expected 3 fields, saw 4\\nSkipping line 7320: expected 3 fields, saw 4\\nSkipping line 7327: expected 3 fields, saw 4\\nSkipping line 7334: expected 3 fields, saw 4\\nSkipping line 7338: expected 3 fields, saw 4\\nSkipping line 7345: expected 3 fields, saw 4\\nSkipping line 7374: expected 3 fields, saw 4\\nSkipping line 7380: expected 3 fields, saw 4\\nSkipping line 7383: expected 3 fields, saw 4\\nSkipping line 7392: expected 3 fields, saw 4\\nSkipping line 7409: expected 3 fields, saw 4\\nSkipping line 7412: expected 3 fields, saw 4\\nSkipping line 7418: expected 3 fields, saw 4\\nSkipping line 7422: expected 3 fields, saw 4\\nSkipping line 7427: expected 3 fields, saw 4\\nSkipping line 7439: expected 3 fields, saw 4\\nSkipping line 7446: expected 3 fields, saw 4\\nSkipping line 7447: expected 3 fields, saw 4\\nSkipping line 7458: expected 3 fields, saw 4\\nSkipping line 7484: expected 3 fields, saw 4\\nSkipping line 7498: expected 3 fields, saw 4\\nSkipping line 7502: expected 3 fields, saw 4\\nSkipping line 7505: expected 3 fields, saw 4\\nSkipping line 7512: expected 3 fields, saw 4\\nSkipping line 7537: expected 3 fields, saw 4\\nSkipping line 7538: expected 3 fields, saw 4\\nSkipping line 7544: expected 3 fields, saw 4\\nSkipping line 7552: expected 3 fields, saw 4\\nSkipping line 7572: expected 3 fields, saw 4\\nSkipping line 7573: expected 3 fields, saw 4\\nSkipping line 7588: expected 3 fields, saw 4\\nSkipping line 7647: expected 3 fields, saw 4\\nSkipping line 7664: expected 3 fields, saw 4\\nSkipping line 7670: expected 3 fields, saw 4\\nSkipping line 7691: expected 3 fields, saw 4\\nSkipping line 7715: expected 3 fields, saw 4\\nSkipping line 7746: expected 3 fields, saw 4\\nSkipping line 7755: expected 3 fields, saw 4\\nSkipping line 7769: expected 3 fields, saw 4\\nSkipping line 7775: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 1422: expected 3 fields, saw 4\\nSkipping line 1461: expected 3 fields, saw 4\\nSkipping line 1468: expected 3 fields, saw 4\\nSkipping line 1507: expected 3 fields, saw 4\\nSkipping line 1547: expected 3 fields, saw 4\\nSkipping line 1728: expected 3 fields, saw 4\\nSkipping line 1738: expected 3 fields, saw 4\\nSkipping line 1742: expected 3 fields, saw 4\\nSkipping line 1847: expected 3 fields, saw 5\\nSkipping line 2082: expected 3 fields, saw 4\\nSkipping line 2125: expected 3 fields, saw 4\\nSkipping line 3860: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 638: expected 3 fields, saw 4\\nSkipping line 966: expected 3 fields, saw 4\\nSkipping line 1890: expected 3 fields, saw 4\\nSkipping line 2601: expected 3 fields, saw 4\\nSkipping line 2602: expected 3 fields, saw 4\\nSkipping line 2606: expected 3 fields, saw 4\\nSkipping line 2609: expected 3 fields, saw 4\\nSkipping line 2611: expected 3 fields, saw 4\\nSkipping line 2627: expected 3 fields, saw 4\\nSkipping line 2642: expected 3 fields, saw 4\\nSkipping line 2647: expected 3 fields, saw 4\\nSkipping line 2656: expected 3 fields, saw 4\\nSkipping line 2662: expected 3 fields, saw 4\\nSkipping line 2663: expected 3 fields, saw 4\\nSkipping line 2664: expected 3 fields, saw 4\\nSkipping line 2665: expected 3 fields, saw 4\\nSkipping line 2667: expected 3 fields, saw 4\\nSkipping line 2670: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 71: expected 3 fields, saw 4\\nSkipping line 88: expected 3 fields, saw 4\\nSkipping line 130: expected 3 fields, saw 4\\nSkipping line 145: expected 3 fields, saw 4\\nSkipping line 257: expected 3 fields, saw 4\\nSkipping line 272: expected 3 fields, saw 4\\nSkipping line 325: expected 3 fields, saw 4\\nSkipping line 401: expected 3 fields, saw 4\\nSkipping line 412: expected 3 fields, saw 4\\nSkipping line 426: expected 3 fields, saw 4\\nSkipping line 507: expected 3 fields, saw 4\\nSkipping line 813: expected 3 fields, saw 4\\nSkipping line 1319: expected 3 fields, saw 4\\nSkipping line 1576: expected 3 fields, saw 4\\nSkipping line 1592: expected 3 fields, saw 4\\nSkipping line 1686: expected 3 fields, saw 4\\nSkipping line 1929: expected 3 fields, saw 4\\nSkipping line 2019: expected 3 fields, saw 4\\nSkipping line 2359: expected 3 fields, saw 4\\nSkipping line 2438: expected 3 fields, saw 4\\nSkipping line 2679: expected 3 fields, saw 4\\nSkipping line 2680: expected 3 fields, saw 4\\nSkipping line 2681: expected 3 fields, saw 4\\nSkipping line 3098: expected 3 fields, saw 4\\nSkipping line 3380: expected 3 fields, saw 4\\nSkipping line 3723: expected 3 fields, saw 4\\nSkipping line 4013: expected 3 fields, saw 4\\nSkipping line 4024: expected 3 fields, saw 4\\nSkipping line 4026: expected 3 fields, saw 4\\nSkipping line 4181: expected 3 fields, saw 4\\nSkipping line 4283: expected 3 fields, saw 4\\nSkipping line 4305: expected 3 fields, saw 4\\nSkipping line 4403: expected 3 fields, saw 4\\nSkipping line 4443: expected 3 fields, saw 4\\nSkipping line 4475: expected 3 fields, saw 4\\nSkipping line 4576: expected 3 fields, saw 4\\nSkipping line 4636: expected 3 fields, saw 4\\nSkipping line 4661: expected 3 fields, saw 4\\nSkipping line 4739: expected 3 fields, saw 4\\nSkipping line 4780: expected 3 fields, saw 4\\nSkipping line 4782: expected 3 fields, saw 4\\nSkipping line 4783: expected 3 fields, saw 4\\nSkipping line 4798: expected 3 fields, saw 4\\nSkipping line 4805: expected 3 fields, saw 4\\nSkipping line 4816: expected 3 fields, saw 4\\nSkipping line 4905: expected 3 fields, saw 4\\nSkipping line 4922: expected 3 fields, saw 4\\nSkipping line 4954: expected 3 fields, saw 4\\nSkipping line 4955: expected 3 fields, saw 4\\nSkipping line 4956: expected 3 fields, saw 4\\nSkipping line 4989: expected 3 fields, saw 4\\nSkipping line 5020: expected 3 fields, saw 4\\nSkipping line 5021: expected 3 fields, saw 4\\nSkipping line 5036: expected 3 fields, saw 4\\nSkipping line 5051: expected 3 fields, saw 4\\nSkipping line 5076: expected 3 fields, saw 4\\nSkipping line 5096: expected 3 fields, saw 4\\nSkipping line 5164: expected 3 fields, saw 4\\nSkipping line 5165: expected 3 fields, saw 4\\nSkipping line 5175: expected 3 fields, saw 4\\nSkipping line 5178: expected 3 fields, saw 4\\nSkipping line 5192: expected 3 fields, saw 4\\nSkipping line 5261: expected 3 fields, saw 4\\nSkipping line 5322: expected 3 fields, saw 4\\nSkipping line 5329: expected 3 fields, saw 4\\nSkipping line 5376: expected 3 fields, saw 4\\nSkipping line 5380: expected 3 fields, saw 4\\nSkipping line 5383: expected 3 fields, saw 4\\nSkipping line 5400: expected 3 fields, saw 4\\nSkipping line 5414: expected 3 fields, saw 4\\nSkipping line 5448: expected 3 fields, saw 4\\nSkipping line 5465: expected 3 fields, saw 4\\nSkipping line 5477: expected 3 fields, saw 4\\nSkipping line 5495: expected 3 fields, saw 4\\nSkipping line 5515: expected 3 fields, saw 4\\nSkipping line 5521: expected 3 fields, saw 4\\nSkipping line 5558: expected 3 fields, saw 4\\nSkipping line 5587: expected 3 fields, saw 4\\nSkipping line 5624: expected 3 fields, saw 4\\nSkipping line 5637: expected 3 fields, saw 4\\nSkipping line 5686: expected 3 fields, saw 4\\nSkipping line 5710: expected 3 fields, saw 4\\nSkipping line 5767: expected 3 fields, saw 4\\nSkipping line 5791: expected 3 fields, saw 4\\nSkipping line 5799: expected 3 fields, saw 4\\nSkipping line 5845: expected 3 fields, saw 4\\nSkipping line 5861: expected 3 fields, saw 4\\nSkipping line 5868: expected 3 fields, saw 4\\nSkipping line 5901: expected 3 fields, saw 4\\nSkipping line 5923: expected 3 fields, saw 4\\nSkipping line 5925: expected 3 fields, saw 4\\nSkipping line 5928: expected 3 fields, saw 4\\nSkipping line 5946: expected 3 fields, saw 4\\nSkipping line 5957: expected 3 fields, saw 4\\nSkipping line 5958: expected 3 fields, saw 4\\nSkipping line 5997: expected 3 fields, saw 4\\nSkipping line 6017: expected 3 fields, saw 4\\nSkipping line 6037: expected 3 fields, saw 4\\n'\n",
      "b'Skipping line 523: expected 3 fields, saw 4\\n'\n"
     ]
    }
   ],
   "source": [
    "file_source_path = './Health-Tweets'\n",
    "all_files = os.listdir(file_source_path)\n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "list_ = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_csv(file_source_path + '/' + file_, index_col=0,\n",
    "                     sep='|', header=None, error_bad_lines=False,\n",
    "                     encoding = \"ISO-8859-1\")\n",
    "    df.columns = ['datetime', 'text']\n",
    "    df['source'] = file_.replace('.txt', '')\n",
    "    list_.append(df)\n",
    "\n",
    "tweets = pd.concat(list_)\n",
    "tweets['text'] = tweets['text'].str.split('http://', 1, expand=True)[0]\n",
    "tweets['text'] = tweets['text'].str.lower()\n",
    "tweets['text'] = tweets['text'] + '<EOS>'\n",
    "tweets['text'] = tweets['text'].str.replace('&amp;', '&')\n",
    "tweets['text'] = tweets['text'].str.replace('&gt;', '>')\n",
    "tweets['text'] = tweets['text'].str.replace('&ls;', '<')\n",
    "tweets['text'] = tweets['text'].str.replace('([$*\\\",\\'%//\\-@&:;!?()#])', r' \\1 ')\n",
    "tweets['text'] = tweets['text'].str.replace(\"(<EOS>)\", r\" \\1 \")\n",
    "tweets['text'] = tweets['text'].str.replace('\\s{2,}', ' ')\n",
    "\n",
    "tweets = tweets.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>586266687948881921</th>\n",
       "      <td>Thu Apr 09 20:37:25 +0000 2015</td>\n",
       "      <td>drugs need careful monitoring for expiry dates...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586266687017771008</th>\n",
       "      <td>Thu Apr 09 20:37:25 +0000 2015</td>\n",
       "      <td>sabra hummus recalled in u.s. &lt;EOS&gt;</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586266685495214080</th>\n",
       "      <td>Thu Apr 09 20:37:24 +0000 2015</td>\n",
       "      <td>u.s. sperm bank sued by canadian couple didn '...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586226316820623360</th>\n",
       "      <td>Thu Apr 09 17:57:00 +0000 2015</td>\n",
       "      <td>manitoba pharmacists want clampdown on tylenol...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586164344452354048</th>\n",
       "      <td>Thu Apr 09 13:50:44 +0000 2015</td>\n",
       "      <td>mom of 7 ' spooked ' by vaccinations reverses ...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586164343701508096</th>\n",
       "      <td>Thu Apr 09 13:50:44 +0000 2015</td>\n",
       "      <td>hamilton police send mental health pros to the...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586143262810845184</th>\n",
       "      <td>Thu Apr 09 12:26:58 +0000 2015</td>\n",
       "      <td>wind turbine noise linked to only 1 health iss...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586104681849425921</th>\n",
       "      <td>Thu Apr 09 09:53:40 +0000 2015</td>\n",
       "      <td>' wild west ' of e - cigarettes sparks debate...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586104681136398337</th>\n",
       "      <td>Thu Apr 09 09:53:39 +0000 2015</td>\n",
       "      <td>dementia patients sold unproven ' brainwave op...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585923818805276672</th>\n",
       "      <td>Wed Apr 08 21:54:58 +0000 2015</td>\n",
       "      <td>passengers on second china - vancouver flight ...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585906983091376128</th>\n",
       "      <td>Wed Apr 08 20:48:05 +0000 2015</td>\n",
       "      <td>check expiry dates , health canada advises aft...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585855618738589696</th>\n",
       "      <td>Wed Apr 08 17:23:58 +0000 2015</td>\n",
       "      <td>hashtagging eating disorders : help or hindran...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585819838821576705</th>\n",
       "      <td>Wed Apr 08 15:01:48 +0000 2015</td>\n",
       "      <td>obama says memory of daughter ' s preschool as...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585731543282376704</th>\n",
       "      <td>Wed Apr 08 09:10:56 +0000 2015</td>\n",
       "      <td>women into healing accused of failing drug add...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585579956371066880</th>\n",
       "      <td>Tue Apr 07 23:08:35 +0000 2015</td>\n",
       "      <td>expired alesse birth control exposes ' deficie...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585513608873934848</th>\n",
       "      <td>Tue Apr 07 18:44:57 +0000 2015</td>\n",
       "      <td>despite paying top dollar , some military ment...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585495067135242240</th>\n",
       "      <td>Tue Apr 07 17:31:16 +0000 2015</td>\n",
       "      <td>boy ' s severe peanut , fish allergies traced ...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585454356285169664</th>\n",
       "      <td>Tue Apr 07 14:49:30 +0000 2015</td>\n",
       "      <td>weight watchers , jenny craig get best marks i...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585454355634991104</th>\n",
       "      <td>Tue Apr 07 14:49:30 +0000 2015</td>\n",
       "      <td>cancer - stricken baby from whitehorse awaitin...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585434893343006720</th>\n",
       "      <td>Tue Apr 07 13:32:10 +0000 2015</td>\n",
       "      <td>avian flu confirmed on turkey farm near woodst...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585434892646776832</th>\n",
       "      <td>Tue Apr 07 13:32:09 +0000 2015</td>\n",
       "      <td>sperm donor ' s criminal record , schizophreni...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585416006467624960</th>\n",
       "      <td>Tue Apr 07 12:17:07 +0000 2015</td>\n",
       "      <td>shoppers drug mart mistakenly sells expired bi...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585416005507129345</th>\n",
       "      <td>Tue Apr 07 12:17:06 +0000 2015</td>\n",
       "      <td>fracking criticism spreads , even in alberta a...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585161891246256130</th>\n",
       "      <td>Mon Apr 06 19:27:21 +0000 2015</td>\n",
       "      <td>fake oxycontin suspected in od death of moose ...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585094682687877120</th>\n",
       "      <td>Mon Apr 06 15:00:17 +0000 2015</td>\n",
       "      <td>b.c. doctor geoffrey harding slain on vacation...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585061324331159552</th>\n",
       "      <td>Mon Apr 06 12:47:44 +0000 2015</td>\n",
       "      <td>titanium implant ' massively ' improves qualit...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585018790229868544</th>\n",
       "      <td>Mon Apr 06 09:58:43 +0000 2015</td>\n",
       "      <td>listen to mom : loud smartphone music can blas...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585018789357424641</th>\n",
       "      <td>Mon Apr 06 09:58:43 +0000 2015</td>\n",
       "      <td>addiction canada ignores government orders to ...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584934347465031680</th>\n",
       "      <td>Mon Apr 06 04:23:10 +0000 2015</td>\n",
       "      <td>breast milk sold online may be contaminated wi...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584761447428685824</th>\n",
       "      <td>Sun Apr 05 16:56:08 +0000 2015</td>\n",
       "      <td>ghana : how canada is ' scaling up ' pediatric...</td>\n",
       "      <td>cbchealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386082454794420224</th>\n",
       "      <td>Fri Oct 04 10:56:44 +0000 2013</td>\n",
       "      <td>part - time workers search new exchanges for h...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386073854529265664</th>\n",
       "      <td>Fri Oct 04 10:22:34 +0000 2013</td>\n",
       "      <td>part - time workers search new exchanges for h...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386068673146138624</th>\n",
       "      <td>Fri Oct 04 10:01:59 +0000 2013</td>\n",
       "      <td>analyzing the politics of affordable care act ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385900391474671616</th>\n",
       "      <td>Thu Oct 03 22:53:17 +0000 2013</td>\n",
       "      <td>some online journals will publish fake science...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385879434412449792</th>\n",
       "      <td>Thu Oct 03 21:30:01 +0000 2013</td>\n",
       "      <td>cdc : shutdown strains foodborne illness track...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385874355890421760</th>\n",
       "      <td>Thu Oct 03 21:09:50 +0000 2013</td>\n",
       "      <td>insurance brokers look for relevance as health...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385845109587460097</th>\n",
       "      <td>Thu Oct 03 19:13:37 +0000 2013</td>\n",
       "      <td>medicaid looks good to a former young invincib...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385802703731171328</th>\n",
       "      <td>Thu Oct 03 16:25:07 +0000 2013</td>\n",
       "      <td>how ' s the sausage made ? these folks really ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385707561263321088</th>\n",
       "      <td>Thu Oct 03 10:07:03 +0000 2013</td>\n",
       "      <td>from therapy dogs to new patients , shutdown a...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385679730131075072</th>\n",
       "      <td>Thu Oct 03 08:16:27 +0000 2013</td>\n",
       "      <td>back to work after a baby , but without health...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385672449540771840</th>\n",
       "      <td>Thu Oct 03 07:47:32 +0000 2013</td>\n",
       "      <td>small businesses may find insurance relief in ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385667293671604224</th>\n",
       "      <td>Thu Oct 03 07:27:02 +0000 2013</td>\n",
       "      <td>studying the science behind child prodigies &lt;E...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385517531894923264</th>\n",
       "      <td>Wed Oct 02 21:31:56 +0000 2013</td>\n",
       "      <td>a deet - like mosquito spray that smells like ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385509563451338753</th>\n",
       "      <td>Wed Oct 02 21:00:17 +0000 2013</td>\n",
       "      <td>fish guidelines for pregnant women may be too ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385501753711816704</th>\n",
       "      <td>Wed Oct 02 20:29:15 +0000 2013</td>\n",
       "      <td>federal funds for meals on wheels tied up in s...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385478815012892672</th>\n",
       "      <td>Wed Oct 02 18:58:06 +0000 2013</td>\n",
       "      <td>is it time to cool it on kale already ? &lt;EOS&gt;</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385467426085036033</th>\n",
       "      <td>Wed Oct 02 18:12:50 +0000 2013</td>\n",
       "      <td>why eye contact can fail to win people over &lt;E...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385441690594865152</th>\n",
       "      <td>Wed Oct 02 16:30:34 +0000 2013</td>\n",
       "      <td>health care act reminds young adults they ' re...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385321705046298624</th>\n",
       "      <td>Wed Oct 02 08:33:48 +0000 2013</td>\n",
       "      <td>tech problems plague first day of health excha...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385321694690566144</th>\n",
       "      <td>Wed Oct 02 08:33:45 +0000 2013</td>\n",
       "      <td>can millet take on quinoa ? first , it ' ll ne...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385312051255865344</th>\n",
       "      <td>Wed Oct 02 07:55:26 +0000 2013</td>\n",
       "      <td>obamacare day one : a tale of two states &lt;EOS&gt;</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385166985799409664</th>\n",
       "      <td>Tue Oct 01 22:19:00 +0000 2013</td>\n",
       "      <td>in florida , insurer and nonprofits work on en...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385163152922804224</th>\n",
       "      <td>Tue Oct 01 22:03:46 +0000 2013</td>\n",
       "      <td>health exchange day one : a view from californ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385163152117485568</th>\n",
       "      <td>Tue Oct 01 22:03:46 +0000 2013</td>\n",
       "      <td>a few snags as health exchange sign up starts ...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385163151098265600</th>\n",
       "      <td>Tue Oct 01 22:03:45 +0000 2013</td>\n",
       "      <td>ill. governor touts health exchange legislatur...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385159101644570624</th>\n",
       "      <td>Tue Oct 01 21:47:40 +0000 2013</td>\n",
       "      <td>first step in health exchange enrollment : tra...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385142492687261697</th>\n",
       "      <td>Tue Oct 01 20:41:40 +0000 2013</td>\n",
       "      <td>puberty is coming earlier , but that doesn ' t...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385132341850800128</th>\n",
       "      <td>Tue Oct 01 20:01:20 +0000 2013</td>\n",
       "      <td>shutdown leaves program feeding women and infa...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385124804841721856</th>\n",
       "      <td>Tue Oct 01 19:31:23 +0000 2013</td>\n",
       "      <td>for middle - aged women , stress may boost alz...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385116867742171137</th>\n",
       "      <td>Tue Oct 01 18:59:51 +0000 2013</td>\n",
       "      <td>for middle - aged women , stress may boost alz...</td>\n",
       "      <td>nprhealth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          datetime  \\\n",
       "0                                                    \n",
       "586266687948881921  Thu Apr 09 20:37:25 +0000 2015   \n",
       "586266687017771008  Thu Apr 09 20:37:25 +0000 2015   \n",
       "586266685495214080  Thu Apr 09 20:37:24 +0000 2015   \n",
       "586226316820623360  Thu Apr 09 17:57:00 +0000 2015   \n",
       "586164344452354048  Thu Apr 09 13:50:44 +0000 2015   \n",
       "586164343701508096  Thu Apr 09 13:50:44 +0000 2015   \n",
       "586143262810845184  Thu Apr 09 12:26:58 +0000 2015   \n",
       "586104681849425921  Thu Apr 09 09:53:40 +0000 2015   \n",
       "586104681136398337  Thu Apr 09 09:53:39 +0000 2015   \n",
       "585923818805276672  Wed Apr 08 21:54:58 +0000 2015   \n",
       "585906983091376128  Wed Apr 08 20:48:05 +0000 2015   \n",
       "585855618738589696  Wed Apr 08 17:23:58 +0000 2015   \n",
       "585819838821576705  Wed Apr 08 15:01:48 +0000 2015   \n",
       "585731543282376704  Wed Apr 08 09:10:56 +0000 2015   \n",
       "585579956371066880  Tue Apr 07 23:08:35 +0000 2015   \n",
       "585513608873934848  Tue Apr 07 18:44:57 +0000 2015   \n",
       "585495067135242240  Tue Apr 07 17:31:16 +0000 2015   \n",
       "585454356285169664  Tue Apr 07 14:49:30 +0000 2015   \n",
       "585454355634991104  Tue Apr 07 14:49:30 +0000 2015   \n",
       "585434893343006720  Tue Apr 07 13:32:10 +0000 2015   \n",
       "585434892646776832  Tue Apr 07 13:32:09 +0000 2015   \n",
       "585416006467624960  Tue Apr 07 12:17:07 +0000 2015   \n",
       "585416005507129345  Tue Apr 07 12:17:06 +0000 2015   \n",
       "585161891246256130  Mon Apr 06 19:27:21 +0000 2015   \n",
       "585094682687877120  Mon Apr 06 15:00:17 +0000 2015   \n",
       "585061324331159552  Mon Apr 06 12:47:44 +0000 2015   \n",
       "585018790229868544  Mon Apr 06 09:58:43 +0000 2015   \n",
       "585018789357424641  Mon Apr 06 09:58:43 +0000 2015   \n",
       "584934347465031680  Mon Apr 06 04:23:10 +0000 2015   \n",
       "584761447428685824  Sun Apr 05 16:56:08 +0000 2015   \n",
       "...                                            ...   \n",
       "386082454794420224  Fri Oct 04 10:56:44 +0000 2013   \n",
       "386073854529265664  Fri Oct 04 10:22:34 +0000 2013   \n",
       "386068673146138624  Fri Oct 04 10:01:59 +0000 2013   \n",
       "385900391474671616  Thu Oct 03 22:53:17 +0000 2013   \n",
       "385879434412449792  Thu Oct 03 21:30:01 +0000 2013   \n",
       "385874355890421760  Thu Oct 03 21:09:50 +0000 2013   \n",
       "385845109587460097  Thu Oct 03 19:13:37 +0000 2013   \n",
       "385802703731171328  Thu Oct 03 16:25:07 +0000 2013   \n",
       "385707561263321088  Thu Oct 03 10:07:03 +0000 2013   \n",
       "385679730131075072  Thu Oct 03 08:16:27 +0000 2013   \n",
       "385672449540771840  Thu Oct 03 07:47:32 +0000 2013   \n",
       "385667293671604224  Thu Oct 03 07:27:02 +0000 2013   \n",
       "385517531894923264  Wed Oct 02 21:31:56 +0000 2013   \n",
       "385509563451338753  Wed Oct 02 21:00:17 +0000 2013   \n",
       "385501753711816704  Wed Oct 02 20:29:15 +0000 2013   \n",
       "385478815012892672  Wed Oct 02 18:58:06 +0000 2013   \n",
       "385467426085036033  Wed Oct 02 18:12:50 +0000 2013   \n",
       "385441690594865152  Wed Oct 02 16:30:34 +0000 2013   \n",
       "385321705046298624  Wed Oct 02 08:33:48 +0000 2013   \n",
       "385321694690566144  Wed Oct 02 08:33:45 +0000 2013   \n",
       "385312051255865344  Wed Oct 02 07:55:26 +0000 2013   \n",
       "385166985799409664  Tue Oct 01 22:19:00 +0000 2013   \n",
       "385163152922804224  Tue Oct 01 22:03:46 +0000 2013   \n",
       "385163152117485568  Tue Oct 01 22:03:46 +0000 2013   \n",
       "385163151098265600  Tue Oct 01 22:03:45 +0000 2013   \n",
       "385159101644570624  Tue Oct 01 21:47:40 +0000 2013   \n",
       "385142492687261697  Tue Oct 01 20:41:40 +0000 2013   \n",
       "385132341850800128  Tue Oct 01 20:01:20 +0000 2013   \n",
       "385124804841721856  Tue Oct 01 19:31:23 +0000 2013   \n",
       "385116867742171137  Tue Oct 01 18:59:51 +0000 2013   \n",
       "\n",
       "                                                                 text  \\\n",
       "0                                                                       \n",
       "586266687948881921  drugs need careful monitoring for expiry dates...   \n",
       "586266687017771008               sabra hummus recalled in u.s. <EOS>    \n",
       "586266685495214080  u.s. sperm bank sued by canadian couple didn '...   \n",
       "586226316820623360  manitoba pharmacists want clampdown on tylenol...   \n",
       "586164344452354048  mom of 7 ' spooked ' by vaccinations reverses ...   \n",
       "586164343701508096  hamilton police send mental health pros to the...   \n",
       "586143262810845184  wind turbine noise linked to only 1 health iss...   \n",
       "586104681849425921   ' wild west ' of e - cigarettes sparks debate...   \n",
       "586104681136398337  dementia patients sold unproven ' brainwave op...   \n",
       "585923818805276672  passengers on second china - vancouver flight ...   \n",
       "585906983091376128  check expiry dates , health canada advises aft...   \n",
       "585855618738589696  hashtagging eating disorders : help or hindran...   \n",
       "585819838821576705  obama says memory of daughter ' s preschool as...   \n",
       "585731543282376704  women into healing accused of failing drug add...   \n",
       "585579956371066880  expired alesse birth control exposes ' deficie...   \n",
       "585513608873934848  despite paying top dollar , some military ment...   \n",
       "585495067135242240  boy ' s severe peanut , fish allergies traced ...   \n",
       "585454356285169664  weight watchers , jenny craig get best marks i...   \n",
       "585454355634991104  cancer - stricken baby from whitehorse awaitin...   \n",
       "585434893343006720  avian flu confirmed on turkey farm near woodst...   \n",
       "585434892646776832  sperm donor ' s criminal record , schizophreni...   \n",
       "585416006467624960  shoppers drug mart mistakenly sells expired bi...   \n",
       "585416005507129345  fracking criticism spreads , even in alberta a...   \n",
       "585161891246256130  fake oxycontin suspected in od death of moose ...   \n",
       "585094682687877120  b.c. doctor geoffrey harding slain on vacation...   \n",
       "585061324331159552  titanium implant ' massively ' improves qualit...   \n",
       "585018790229868544  listen to mom : loud smartphone music can blas...   \n",
       "585018789357424641  addiction canada ignores government orders to ...   \n",
       "584934347465031680  breast milk sold online may be contaminated wi...   \n",
       "584761447428685824  ghana : how canada is ' scaling up ' pediatric...   \n",
       "...                                                               ...   \n",
       "386082454794420224  part - time workers search new exchanges for h...   \n",
       "386073854529265664  part - time workers search new exchanges for h...   \n",
       "386068673146138624  analyzing the politics of affordable care act ...   \n",
       "385900391474671616  some online journals will publish fake science...   \n",
       "385879434412449792  cdc : shutdown strains foodborne illness track...   \n",
       "385874355890421760  insurance brokers look for relevance as health...   \n",
       "385845109587460097  medicaid looks good to a former young invincib...   \n",
       "385802703731171328  how ' s the sausage made ? these folks really ...   \n",
       "385707561263321088  from therapy dogs to new patients , shutdown a...   \n",
       "385679730131075072  back to work after a baby , but without health...   \n",
       "385672449540771840  small businesses may find insurance relief in ...   \n",
       "385667293671604224  studying the science behind child prodigies <E...   \n",
       "385517531894923264  a deet - like mosquito spray that smells like ...   \n",
       "385509563451338753  fish guidelines for pregnant women may be too ...   \n",
       "385501753711816704  federal funds for meals on wheels tied up in s...   \n",
       "385478815012892672     is it time to cool it on kale already ? <EOS>    \n",
       "385467426085036033  why eye contact can fail to win people over <E...   \n",
       "385441690594865152  health care act reminds young adults they ' re...   \n",
       "385321705046298624  tech problems plague first day of health excha...   \n",
       "385321694690566144  can millet take on quinoa ? first , it ' ll ne...   \n",
       "385312051255865344    obamacare day one : a tale of two states <EOS>    \n",
       "385166985799409664  in florida , insurer and nonprofits work on en...   \n",
       "385163152922804224  health exchange day one : a view from californ...   \n",
       "385163152117485568  a few snags as health exchange sign up starts ...   \n",
       "385163151098265600  ill. governor touts health exchange legislatur...   \n",
       "385159101644570624  first step in health exchange enrollment : tra...   \n",
       "385142492687261697  puberty is coming earlier , but that doesn ' t...   \n",
       "385132341850800128  shutdown leaves program feeding women and infa...   \n",
       "385124804841721856  for middle - aged women , stress may boost alz...   \n",
       "385116867742171137  for middle - aged women , stress may boost alz...   \n",
       "\n",
       "                       source  \n",
       "0                              \n",
       "586266687948881921  cbchealth  \n",
       "586266687017771008  cbchealth  \n",
       "586266685495214080  cbchealth  \n",
       "586226316820623360  cbchealth  \n",
       "586164344452354048  cbchealth  \n",
       "586164343701508096  cbchealth  \n",
       "586143262810845184  cbchealth  \n",
       "586104681849425921  cbchealth  \n",
       "586104681136398337  cbchealth  \n",
       "585923818805276672  cbchealth  \n",
       "585906983091376128  cbchealth  \n",
       "585855618738589696  cbchealth  \n",
       "585819838821576705  cbchealth  \n",
       "585731543282376704  cbchealth  \n",
       "585579956371066880  cbchealth  \n",
       "585513608873934848  cbchealth  \n",
       "585495067135242240  cbchealth  \n",
       "585454356285169664  cbchealth  \n",
       "585454355634991104  cbchealth  \n",
       "585434893343006720  cbchealth  \n",
       "585434892646776832  cbchealth  \n",
       "585416006467624960  cbchealth  \n",
       "585416005507129345  cbchealth  \n",
       "585161891246256130  cbchealth  \n",
       "585094682687877120  cbchealth  \n",
       "585061324331159552  cbchealth  \n",
       "585018790229868544  cbchealth  \n",
       "585018789357424641  cbchealth  \n",
       "584934347465031680  cbchealth  \n",
       "584761447428685824  cbchealth  \n",
       "...                       ...  \n",
       "386082454794420224  nprhealth  \n",
       "386073854529265664  nprhealth  \n",
       "386068673146138624  nprhealth  \n",
       "385900391474671616  nprhealth  \n",
       "385879434412449792  nprhealth  \n",
       "385874355890421760  nprhealth  \n",
       "385845109587460097  nprhealth  \n",
       "385802703731171328  nprhealth  \n",
       "385707561263321088  nprhealth  \n",
       "385679730131075072  nprhealth  \n",
       "385672449540771840  nprhealth  \n",
       "385667293671604224  nprhealth  \n",
       "385517531894923264  nprhealth  \n",
       "385509563451338753  nprhealth  \n",
       "385501753711816704  nprhealth  \n",
       "385478815012892672  nprhealth  \n",
       "385467426085036033  nprhealth  \n",
       "385441690594865152  nprhealth  \n",
       "385321705046298624  nprhealth  \n",
       "385321694690566144  nprhealth  \n",
       "385312051255865344  nprhealth  \n",
       "385166985799409664  nprhealth  \n",
       "385163152922804224  nprhealth  \n",
       "385163152117485568  nprhealth  \n",
       "385163151098265600  nprhealth  \n",
       "385159101644570624  nprhealth  \n",
       "385142492687261697  nprhealth  \n",
       "385132341850800128  nprhealth  \n",
       "385124804841721856  nprhealth  \n",
       "385116867742171137  nprhealth  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ' '.join(tweets.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going by Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = sorted(list(set(raw_text)))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# n_chars = len(raw_text)\n",
    "# n_vocab = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n"
     ]
    }
   ],
   "source": [
    "# seq_length = 100\n",
    "# dataX =  \n",
    "# dataY = []\n",
    "# for i in range(0, n_chars - seq_length, 1):\n",
    "#     seq_in = raw_text[i:i + seq_length]\n",
    "#     seq_out = raw_text[i + seq_length]\n",
    "#     dataX.append([char_to_int[char] for char in seq_in])\n",
    "#     dataY.append(char_to_int[seq_out])\n",
    "#     if i % 100000 == 0: print(i)\n",
    "# n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "794984"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# x = x / float(n_vocab)\n",
    "# y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4ee10d1c7946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(LSTM(256))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[i], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_split = raw_text.split()\n",
    "\n",
    "word_counter = Counter(word_split)\n",
    "\n",
    "word_split = [word if word_counter[word] > 20 else '<UNK>' for word in word_split]\n",
    "words = sorted(list(set(word_split)))\n",
    "\n",
    "word_to_int = dict((w, i) for i, w in enumerate(words))\n",
    "n_words = len(word_split)\n",
    "n_vocab = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 20\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_words - seq_length, 1):\n",
    "    seq_in = word_split[i:i + seq_length]\n",
    "    seq_out = word_split[i + seq_length]\n",
    "    dataX.append([word_to_int[word] for word in seq_in])\n",
    "    dataY.append(word_to_int[seq_out])\n",
    "    if i % 100000 == 0: print(i)\n",
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140655, 20, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "x = x / float(n_vocab)\n",
    "y = np_utils.to_categorical(dataY)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_words = Sequential()\n",
    "model_words.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "model_words.add(Dropout(0.2))\n",
    "model_words.add(LSTM(256))\n",
    "model_words.add(Dropout(0.2))\n",
    "model_words.add(Dense(y.shape[1], activation='softmax'))\n",
    "model_words.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "140655/140655 [==============================] - 232s 2ms/step - loss: 4.5334\n",
      "\n",
      "Epoch 00001: loss improved from inf to 4.53339, saving model to weights-lstm-improvement-01-4.5334-bigger.hdfs\n",
      "Epoch 2/50\n",
      "140655/140655 [==============================] - 211s 1ms/step - loss: 4.4463\n",
      "\n",
      "Epoch 00002: loss improved from 4.53339 to 4.44632, saving model to weights-lstm-improvement-02-4.4463-bigger.hdfs\n",
      "Epoch 3/50\n",
      "140655/140655 [==============================] - 211s 2ms/step - loss: 4.3217\n",
      "\n",
      "Epoch 00003: loss improved from 4.44632 to 4.32166, saving model to weights-lstm-improvement-03-4.3217-bigger.hdfs\n",
      "Epoch 4/50\n",
      "140655/140655 [==============================] - 212s 2ms/step - loss: 4.2418\n",
      "\n",
      "Epoch 00004: loss improved from 4.32166 to 4.24183, saving model to weights-lstm-improvement-04-4.2418-bigger.hdfs\n",
      "Epoch 5/50\n",
      "140655/140655 [==============================] - 4037s 29ms/step - loss: 4.1982\n",
      "\n",
      "Epoch 00005: loss improved from 4.24183 to 4.19825, saving model to weights-lstm-improvement-05-4.1982-bigger.hdfs\n",
      "Epoch 6/50\n",
      "140655/140655 [==============================] - 718s 5ms/step - loss: 4.1653\n",
      "\n",
      "Epoch 00006: loss improved from 4.19825 to 4.16530, saving model to weights-lstm-improvement-06-4.1653-bigger.hdfs\n",
      "Epoch 7/50\n",
      "140655/140655 [==============================] - 303s 2ms/step - loss: 4.1255\n",
      "\n",
      "Epoch 00007: loss improved from 4.16530 to 4.12545, saving model to weights-lstm-improvement-07-4.1255-bigger.hdfs\n",
      "Epoch 8/50\n",
      "140655/140655 [==============================] - 886s 6ms/step - loss: 4.0849\n",
      "\n",
      "Epoch 00008: loss improved from 4.12545 to 4.08494, saving model to weights-lstm-improvement-08-4.0849-bigger.hdfs\n",
      "Epoch 9/50\n",
      "140655/140655 [==============================] - 303s 2ms/step - loss: 4.0461\n",
      "\n",
      "Epoch 00009: loss improved from 4.08494 to 4.04605, saving model to weights-lstm-improvement-09-4.0461-bigger.hdfs\n",
      "Epoch 10/50\n",
      "140655/140655 [==============================] - 688s 5ms/step - loss: 4.0098\n",
      "\n",
      "Epoch 00010: loss improved from 4.04605 to 4.00978, saving model to weights-lstm-improvement-10-4.0098-bigger.hdfs\n",
      "Epoch 11/50\n",
      "140655/140655 [==============================] - 965s 7ms/step - loss: 3.9778\n",
      "\n",
      "Epoch 00011: loss improved from 4.00978 to 3.97780, saving model to weights-lstm-improvement-11-3.9778-bigger.hdfs\n",
      "Epoch 12/50\n",
      "140655/140655 [==============================] - 237s 2ms/step - loss: 3.9469\n",
      "\n",
      "Epoch 00012: loss improved from 3.97780 to 3.94689, saving model to weights-lstm-improvement-12-3.9469-bigger.hdfs\n",
      "Epoch 13/50\n",
      "140655/140655 [==============================] - 227s 2ms/step - loss: 3.9149\n",
      "\n",
      "Epoch 00013: loss improved from 3.94689 to 3.91494, saving model to weights-lstm-improvement-13-3.9149-bigger.hdfs\n",
      "Epoch 14/50\n",
      "140655/140655 [==============================] - 221s 2ms/step - loss: 3.8832\n",
      "\n",
      "Epoch 00014: loss improved from 3.91494 to 3.88315, saving model to weights-lstm-improvement-14-3.8832-bigger.hdfs\n",
      "Epoch 15/50\n",
      "140655/140655 [==============================] - 219s 2ms/step - loss: 3.8530\n",
      "\n",
      "Epoch 00015: loss improved from 3.88315 to 3.85301, saving model to weights-lstm-improvement-15-3.8530-bigger.hdfs\n",
      "Epoch 16/50\n",
      "140655/140655 [==============================] - 219s 2ms/step - loss: 3.8212\n",
      "\n",
      "Epoch 00016: loss improved from 3.85301 to 3.82122, saving model to weights-lstm-improvement-16-3.8212-bigger.hdfs\n",
      "Epoch 17/50\n",
      "140655/140655 [==============================] - 232s 2ms/step - loss: 3.7882\n",
      "\n",
      "Epoch 00017: loss improved from 3.82122 to 3.78822, saving model to weights-lstm-improvement-17-3.7882-bigger.hdfs\n",
      "Epoch 18/50\n",
      "140655/140655 [==============================] - 221s 2ms/step - loss: 3.7546\n",
      "\n",
      "Epoch 00018: loss improved from 3.78822 to 3.75465, saving model to weights-lstm-improvement-18-3.7546-bigger.hdfs\n",
      "Epoch 19/50\n",
      "140655/140655 [==============================] - 200s 1ms/step - loss: 3.7220\n",
      "\n",
      "Epoch 00019: loss improved from 3.75465 to 3.72199, saving model to weights-lstm-improvement-19-3.7220-bigger.hdfs\n",
      "Epoch 20/50\n",
      "140655/140655 [==============================] - 200s 1ms/step - loss: 3.6925\n",
      "\n",
      "Epoch 00020: loss improved from 3.72199 to 3.69253, saving model to weights-lstm-improvement-20-3.6925-bigger.hdfs\n",
      "Epoch 21/50\n",
      "140655/140655 [==============================] - 319s 2ms/step - loss: 3.6587\n",
      "\n",
      "Epoch 00021: loss improved from 3.69253 to 3.65873, saving model to weights-lstm-improvement-21-3.6587-bigger.hdfs\n",
      "Epoch 22/50\n",
      "140655/140655 [==============================] - 237s 2ms/step - loss: 3.6277\n",
      "\n",
      "Epoch 00022: loss improved from 3.65873 to 3.62770, saving model to weights-lstm-improvement-22-3.6277-bigger.hdfs\n",
      "Epoch 23/50\n",
      "140655/140655 [==============================] - 224s 2ms/step - loss: 3.5938\n",
      "\n",
      "Epoch 00023: loss improved from 3.62770 to 3.59380, saving model to weights-lstm-improvement-23-3.5938-bigger.hdfs\n",
      "Epoch 24/50\n",
      "140655/140655 [==============================] - 208s 1ms/step - loss: 3.5636\n",
      "\n",
      "Epoch 00024: loss improved from 3.59380 to 3.56361, saving model to weights-lstm-improvement-24-3.5636-bigger.hdfs\n",
      "Epoch 25/50\n",
      "140655/140655 [==============================] - 208s 1ms/step - loss: 3.5381\n",
      "\n",
      "Epoch 00025: loss improved from 3.56361 to 3.53814, saving model to weights-lstm-improvement-25-3.5381-bigger.hdfs\n",
      "Epoch 26/50\n",
      "140655/140655 [==============================] - 377s 3ms/step - loss: 3.5087\n",
      "\n",
      "Epoch 00026: loss improved from 3.53814 to 3.50865, saving model to weights-lstm-improvement-26-3.5087-bigger.hdfs\n",
      "Epoch 27/50\n",
      "140655/140655 [==============================] - 222s 2ms/step - loss: 3.4774\n",
      "\n",
      "Epoch 00027: loss improved from 3.50865 to 3.47740, saving model to weights-lstm-improvement-27-3.4774-bigger.hdfs\n",
      "Epoch 28/50\n",
      "140655/140655 [==============================] - 232s 2ms/step - loss: 3.4508\n",
      "\n",
      "Epoch 00028: loss improved from 3.47740 to 3.45080, saving model to weights-lstm-improvement-28-3.4508-bigger.hdfs\n",
      "Epoch 29/50\n",
      "140655/140655 [==============================] - 236s 2ms/step - loss: 3.4214\n",
      "\n",
      "Epoch 00029: loss improved from 3.45080 to 3.42136, saving model to weights-lstm-improvement-29-3.4214-bigger.hdfs\n",
      "Epoch 30/50\n",
      "140655/140655 [==============================] - 233s 2ms/step - loss: 3.3957\n",
      "\n",
      "Epoch 00030: loss improved from 3.42136 to 3.39572, saving model to weights-lstm-improvement-30-3.3957-bigger.hdfs\n",
      "Epoch 31/50\n",
      "140655/140655 [==============================] - 230s 2ms/step - loss: 3.3709\n",
      "\n",
      "Epoch 00031: loss improved from 3.39572 to 3.37089, saving model to weights-lstm-improvement-31-3.3709-bigger.hdfs\n",
      "Epoch 32/50\n",
      "140655/140655 [==============================] - 231s 2ms/step - loss: 3.3481\n",
      "\n",
      "Epoch 00032: loss improved from 3.37089 to 3.34812, saving model to weights-lstm-improvement-32-3.3481-bigger.hdfs\n",
      "Epoch 33/50\n",
      "140655/140655 [==============================] - 230s 2ms/step - loss: 3.3275\n",
      "\n",
      "Epoch 00033: loss improved from 3.34812 to 3.32750, saving model to weights-lstm-improvement-33-3.3275-bigger.hdfs\n",
      "Epoch 34/50\n",
      "140655/140655 [==============================] - 222s 2ms/step - loss: 3.2993\n",
      "\n",
      "Epoch 00034: loss improved from 3.32750 to 3.29925, saving model to weights-lstm-improvement-34-3.2993-bigger.hdfs\n",
      "Epoch 35/50\n",
      "140655/140655 [==============================] - 221s 2ms/step - loss: 3.2806\n",
      "\n",
      "Epoch 00035: loss improved from 3.29925 to 3.28059, saving model to weights-lstm-improvement-35-3.2806-bigger.hdfs\n",
      "Epoch 36/50\n",
      "140655/140655 [==============================] - 222s 2ms/step - loss: 3.2560\n",
      "\n",
      "Epoch 00036: loss improved from 3.28059 to 3.25597, saving model to weights-lstm-improvement-36-3.2560-bigger.hdfs\n",
      "Epoch 37/50\n",
      "140655/140655 [==============================] - 223s 2ms/step - loss: 3.2413\n",
      "\n",
      "Epoch 00037: loss improved from 3.25597 to 3.24131, saving model to weights-lstm-improvement-37-3.2413-bigger.hdfs\n",
      "Epoch 38/50\n",
      "140655/140655 [==============================] - 222s 2ms/step - loss: 3.2163\n",
      "\n",
      "Epoch 00038: loss improved from 3.24131 to 3.21634, saving model to weights-lstm-improvement-38-3.2163-bigger.hdfs\n",
      "Epoch 39/50\n",
      "140655/140655 [==============================] - 225s 2ms/step - loss: 3.1980\n",
      "\n",
      "Epoch 00039: loss improved from 3.21634 to 3.19796, saving model to weights-lstm-improvement-39-3.1980-bigger.hdfs\n",
      "Epoch 40/50\n",
      "140655/140655 [==============================] - 226s 2ms/step - loss: 3.1792\n",
      "\n",
      "Epoch 00040: loss improved from 3.19796 to 3.17918, saving model to weights-lstm-improvement-40-3.1792-bigger.hdfs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50\n",
      "140655/140655 [==============================] - 210s 1ms/step - loss: 3.1609\n",
      "\n",
      "Epoch 00041: loss improved from 3.17918 to 3.16092, saving model to weights-lstm-improvement-41-3.1609-bigger.hdfs\n",
      "Epoch 42/50\n",
      "140655/140655 [==============================] - 210s 1ms/step - loss: 3.1484\n",
      "\n",
      "Epoch 00042: loss improved from 3.16092 to 3.14843, saving model to weights-lstm-improvement-42-3.1484-bigger.hdfs\n",
      "Epoch 43/50\n",
      "140655/140655 [==============================] - 211s 2ms/step - loss: 3.1280\n",
      "\n",
      "Epoch 00043: loss improved from 3.14843 to 3.12800, saving model to weights-lstm-improvement-43-3.1280-bigger.hdfs\n",
      "Epoch 44/50\n",
      "140655/140655 [==============================] - 212s 2ms/step - loss: 3.1103\n",
      "\n",
      "Epoch 00044: loss improved from 3.12800 to 3.11029, saving model to weights-lstm-improvement-44-3.1103-bigger.hdfs\n",
      "Epoch 45/50\n",
      "140655/140655 [==============================] - 212s 2ms/step - loss: 3.0967\n",
      "\n",
      "Epoch 00045: loss improved from 3.11029 to 3.09665, saving model to weights-lstm-improvement-45-3.0967-bigger.hdfs\n",
      "Epoch 46/50\n",
      "140655/140655 [==============================] - 214s 2ms/step - loss: 3.0828\n",
      "\n",
      "Epoch 00046: loss improved from 3.09665 to 3.08282, saving model to weights-lstm-improvement-46-3.0828-bigger.hdfs\n",
      "Epoch 47/50\n",
      "140655/140655 [==============================] - 219s 2ms/step - loss: 3.0695\n",
      "\n",
      "Epoch 00047: loss improved from 3.08282 to 3.06952, saving model to weights-lstm-improvement-47-3.0695-bigger.hdfs\n",
      "Epoch 48/50\n",
      "140655/140655 [==============================] - 220s 2ms/step - loss: 3.0511\n",
      "\n",
      "Epoch 00048: loss improved from 3.06952 to 3.05112, saving model to weights-lstm-improvement-48-3.0511-bigger.hdfs\n",
      "Epoch 49/50\n",
      "140655/140655 [==============================] - 219s 2ms/step - loss: 3.0358\n",
      "\n",
      "Epoch 00049: loss improved from 3.05112 to 3.03579, saving model to weights-lstm-improvement-49-3.0358-bigger.hdfs\n",
      "Epoch 50/50\n",
      "140655/140655 [==============================] - 219s 2ms/step - loss: 3.0268\n",
      "\n",
      "Epoch 00050: loss improved from 3.03579 to 3.02682, saving model to weights-lstm-improvement-50-3.0268-bigger.hdfs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1b725ba8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = 'weights-lstm-improvement-{epoch:02d}-{loss:.4f}-bigger.hdfs'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model_words.fit(x, y, epochs = 50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" , chemo 4 financial 4 low costly 30 he 5 device 2nd 4 4 2nd 4 4 7 aids advocates \"\n",
      "found kills maker 000 gay dead ford growth facebook ford maker kills doing devices diagnosed kills kills african devices concern kills 000 atlanta facebook maker cure african bkesling against kills gop animals 000 malaria doing drugs. brian against ford kills maker cure cure kills ford 000 atlanta facebook maker kills cure cure kills ford 000 african facebook ford ford kills cure cure kills ford electronic african bkesling against kills gop animals devices maker kills maker devices cure maker ford maker cure cure ford maker 000 bkesling facebook kills ford maker 000 bkesling facebook ford ford maker cure 000 bkesling maker maker 000 african bkesling found found kills kills cure cure maker kills devices animals gay maker maker cure cure ford facebook ford 000 bkesling devices kills kills cure devices ford maker 000 atlanta dead ford maker maker african african malaria maker 000 billion kills facebook cure facebook ford ford ford \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "int_to_word = dict((i, w) for i, w in enumerate(words))\n",
    "\n",
    "model_words = Sequential()\n",
    "model_words.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "model_words.add(Dropout(0.2))\n",
    "model_words.add(LSTM(256))\n",
    "model_words.add(Dropout(0.2))\n",
    "model_words.add(Dense(y.shape[1], activation='softmax'))\n",
    "filename = \"weights-lstm-improvement-50-3.0268-bigger.hdfs\"\n",
    "model_words.load_weights(filename)\n",
    "model_words.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "start = np.random.randint(0, len(dataX) - 1)\n",
    "pattern = dataX[start]\n",
    "print('Seed:')\n",
    "print('\\\"', ' '.join([int_to_word[value] for value in pattern]), '\\\"')\n",
    "\n",
    "for i in range(150):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model_words.predict(x, verbose=0)\n",
    "    index = np.argsort(prediction)\n",
    "    result = int_to_word[index[0, 1]]\n",
    "    seq_in = [int_to_word[value] for value in pattern]\n",
    "    sys.stdout.write(result + ' ')\n",
    "    pattern.append(index[0, 1])\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print('\\nDone.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'devices'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "x = x / float(n_vocab)\n",
    "prediction = model_words.predict(x, verbose=0)\n",
    "index = np.argsort(prediction)\n",
    "\n",
    "int_to_word[index[0, 3]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
